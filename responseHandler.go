package deepseek

import (
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"
)

type ChatCompletionResponse struct {
	ID                string   `json:"id"`                           // Unique identifier for the chat completion.
	Object            string   `json:"object"`                       // Type of the object, typically "chat.completion".
	Created           int64    `json:"created"`                      // Timestamp when the chat completion was created.
	Model             string   `json:"model"`                        // The model used for generating the completion.
	Choices           []Choice `json:"choices"`                      // List of completion choices generated by the model.
	Usage             Usage    `json:"usage"`                        // Token usage statistics.
	SystemFingerprint *string  `json:"system_fingerprint,omitempty"` // Fingerprint of the system configuration.
}

type Choice struct {
	Index        int       `json:"index"`              // Index of the choice in the list of choices.
	Message      Message   `json:"message"`            // The message generated by the model.
	LogProbs     *LogProbs `json:"logprobs,omitempty"` // Log probabilities of the tokens, if available.
	FinishReason string    `json:"finish_reason"`      // Reason why the completion finished.
}

type Message struct {
	Role             string `json:"role"`                        // Role of the message sender (e.g., "user", "assistant").
	Content          string `json:"content"`                     // Content of the message.
	ReasoningContent string `json:"reasoning_content,omitempty"` // Optional reasoning content.
}

type LogProbs struct {
	Tokens        []string             `json:"tokens,omitempty"`         // List of tokens.
	TokenLogProbs []float64            `json:"token_logprobs,omitempty"` // Log probabilities of each token.
	TopLogProbs   []map[string]float64 `json:"top_logprobs,omitempty"`   // Top log probabilities for each token.
}

type Usage struct {
	PromptTokens          int `json:"prompt_tokens"`            // Number of tokens used in the prompt.
	CompletionTokens      int `json:"completion_tokens"`        // Number of tokens used in the completion.
	TotalTokens           int `json:"total_tokens"`             // Total number of tokens used.
	PromptCacheHitTokens  int `json:"prompt_cache_hit_tokens"`  // Number of tokens served from cache.
	PromptCacheMissTokens int `json:"prompt_cache_miss_tokens"` // Number of tokens not served from cache.
}

func HandleChatCompletionResponse(resp *http.Response) (*ChatCompletionResponse, error) {
	body, err := io.ReadAll(resp.Body) //Do not re read the body hereafter.
	if err != nil {
		return nil, fmt.Errorf("failed to read response body: %w", err)
	}

	var parsedResponse ChatCompletionResponse
	if err := json.Unmarshal(body, &parsedResponse); err != nil {
		return nil, handleAPIError(body)
	}
	return &parsedResponse, nil
}

func handleAPIError(body []byte) error {
	responseBody := string(body)

	if len(responseBody) == 0 {
		return fmt.Errorf("failed to parse response JSON: empty response body")
	}
	if strings.HasPrefix(responseBody, "<!DOCTYPE html>") {
		return fmt.Errorf("unexpected HTML response (model may not exist). This is likely an issue with the how some external servers return html responses for error. Make sure you are calling the right path or models")
	}
	return fmt.Errorf("failed to parse response JSON: unexpected end of JSON input. %s", responseBody)
}

package handlers

import (
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"
)

type ChatCompletionResponse struct {
	ID                string   `json:"id"`                 // Unique identifier for the chat completion.
	Object            string   `json:"object"`             // Type of the object, typically "chat.completion".
	Created           int64    `json:"created"`            // Timestamp when the chat completion was created.
	Model             string   `json:"model"`              // The model used for generating the completion.
	Choices           []Choice `json:"choices"`            // List of completion choices generated by the model.
	Usage             Usage    `json:"usage"`              // Token usage statistics.
	SystemFingerprint string   `json:"system_fingerprint"` // Fingerprint of the system configuration.
}

type Choice struct {
	Index        int       `json:"index"`              // Index of the choice in the list of choices.
	Message      Message   `json:"message"`            // The message generated by the model.
	LogProbs     *LogProbs `json:"logprobs,omitempty"` // Log probabilities of the tokens, if available.
	FinishReason string    `json:"finish_reason"`      // Reason why the completion finished.
}

type Message struct {
	Role             string `json:"role"`                        // Role of the message sender (e.g., "user", "assistant").
	Content          string `json:"content"`                     // Content of the message.
	ReasoningContent string `json:"reasoning_content,omitempty"` // Optional reasoning content.
}

type LogProbs struct {
	Tokens        []string             `json:"tokens,omitempty"`         // List of tokens.
	TokenLogProbs []float64            `json:"token_logprobs,omitempty"` // Log probabilities of each token.
	TopLogProbs   []map[string]float64 `json:"top_logprobs,omitempty"`   // Top log probabilities for each token.
}

type Usage struct {
	PromptTokens          int `json:"prompt_tokens"`            // Number of tokens used in the prompt.
	CompletionTokens      int `json:"completion_tokens"`        // Number of tokens used in the completion.
	TotalTokens           int `json:"total_tokens"`             // Total number of tokens used.
	PromptCacheHitTokens  int `json:"prompt_cache_hit_tokens"`  // Number of tokens served from cache.
	PromptCacheMissTokens int `json:"prompt_cache_miss_tokens"` // Number of tokens not served from cache.
}

func HandleChatCompletionResponse(resp *http.Response) (*ChatCompletionResponse, error) {
	body, err := io.ReadAll(resp.Body) //Do not re read the body hereafter.
	if err != nil {
		return nil, fmt.Errorf("failed to read response body: %w", err)
	}

	var parsedResponse ChatCompletionResponse
	if err := json.Unmarshal(body, &parsedResponse); err != nil {
		return nil, handleAPIError(resp)
	}
	return &parsedResponse, nil
}

func handleAPIError(resp *http.Response) error {
	defer func() { _ = resp.Body.Close() }()
	body, _ := io.ReadAll(resp.Body)
	responseBody := string(body)

	if strings.HasPrefix(responseBody, "<html>") {
		return fmt.Errorf("unexpected HTML response (model may not exist). This is likely an issue with the how some external servers return html responses for error")

	}
	var apiResponse struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
	}

	err := json.Unmarshal(body, &apiResponse)
	if err == nil && apiResponse.Code != 0 {
		return fmt.Errorf("parsing error %v: %s", apiResponse.Code, apiResponse.Message)
	}
	return fmt.Errorf("parsing error %d: %s", resp.StatusCode, responseBody)
}
